{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ManifoldLearning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielMartinAlarcon/DS-Sprint-01-Dealing-With-Data/blob/master/module1-afirstlookatdata/ManifoldLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "1QUBvAG0EdDv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This example is based on the sklearn \"[Manifold learning on Handwriting](http://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html)\" example. Let's start by loading some libraries and data.\n",
        "\n",
        "This example was prepared by Yusuke Tomoto and Kyle McDonald for their\n",
        "\"Machine learning for ART\" workshop at Rhizomatiks Research in\n",
        "January, 2016."
      ]
    },
    {
      "metadata": {
        "id": "irp5e_zTEdDw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import offsetbox\n",
        "from sklearn import manifold, datasets, decomposition, ensemble, discriminant_analysis, random_projection\n",
        "%matplotlib inline\n",
        "\n",
        "digits = datasets.load_digits(n_class=6)\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "n_samples, n_features = X.shape\n",
        "n_neighbors = 30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rXsMa3w-EdDz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The data we loaded is a subset of MNIST, which contains 70k handwritten digits. We're only using around 1,000 digits. Here's what they look like:"
      ]
    },
    {
      "metadata": {
        "id": "MB3aQfdyEdDz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_img_per_row = 32\n",
        "img = np.zeros((10 * n_img_per_row, 10 * n_img_per_row))\n",
        "for i in range(n_img_per_row):\n",
        "    ix = 10 * i + 1\n",
        "    for j in range(n_img_per_row):\n",
        "        iy = 10 * j + 1\n",
        "        img[ix:ix + 8, iy:iy + 8] = X[i * n_img_per_row + j].reshape((8, 8))\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.imshow(img, cmap=plt.cm.binary)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7OI4jWrZEdD4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Manifold learning is about discovering a low-dimensional structure (also called a projection, decomposition, manifold, or embedding) for high-dimensional data.\n",
        "\n",
        "Linear dimensionality reduction is the process of finding a projection. If your data existed in three dimensions and you wanted a two dimensional view, this would be similar to rotating or skewing the data until the shadow looked informative. PCA, LDA, and ICA find a rotation and skewing of the data that gives a good projection.\n",
        "\n",
        "Before we look at these projections, we need to define a helper function that will plot the results of our projection."
      ]
    },
    {
      "metadata": {
        "id": "2YnC_PXVEdD5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_embedding(X, y):\n",
        "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
        "    X = (X - x_min) / (x_max - x_min)\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    ax = plt.subplot(111)\n",
        "    for i in range(X.shape[0]):\n",
        "        plt.text(X[i, 0], X[i, 1], str(digits.target[i]),\n",
        "                 color=plt.cm.Set1(y[i] / 10.),\n",
        "                 fontdict={'size': 8})\n",
        "\n",
        "    if hasattr(offsetbox, 'AnnotationBbox'):\n",
        "        # only print thumbnails with matplotlib > 1.0\n",
        "        shown_images = np.array([[1., 1.]])  # just something big\n",
        "        for i in range(digits.data.shape[0]):\n",
        "            dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
        "            if np.min(dist) < 4e-3:\n",
        "                # don't show points that are too close\n",
        "                continue\n",
        "            shown_images = np.r_[shown_images, [X[i]]]\n",
        "            imagebox = offsetbox.AnnotationBbox(\n",
        "                offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r),\n",
        "                X[i])\n",
        "            ax.add_artist(imagebox)\n",
        "            \n",
        "    plt.xticks([]), plt.yticks([])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "THojTqSBEdD8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can find a projection and see what it looks like. The most common one is principle components analysis (PCA)."
      ]
    },
    {
      "metadata": {
        "id": "dGdSIae9EdD9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_pca = decomposition.TruncatedSVD(n_components=2).fit_transform(X)\n",
        "plot_embedding(X_pca, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "00Ezu7KoEdEA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "PCA is good at finding the main axes of variation in a dataset. Here it looks like it's separated the digits a little bit, but it's still hard to see the boundaries between them. One the other hand, linear discriminant analysis (LDA) will find a projection that maximally separates all the classes."
      ]
    },
    {
      "metadata": {
        "id": "DsfR_5gjEdEB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X2 = X.copy()\n",
        "X2.flat[::X.shape[1] + 1] += 0.01  # Make X invertible\n",
        "X_lda = discriminant_analysis.LinearDiscriminantAnalysis(n_components=2).fit_transform(X2, y)\n",
        "plot_embedding(X_lda, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4omVLEfHEdEG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "LDA can have great results, but if you look at the code it says `fit_transform(X2, y)` while PCA says `fit_transform(X)`. This means for LDA you need labeled data (supervised learning), but for PCA you can use unlabaled data (unsupervised learning). Here's a picture explaining the difference:\n",
        "\n",
        "### PCA\n",
        "[![](https://sebastianraschka.com/images/faq/lda-vs-pca/pca.png)](http://sebastianraschka.com/faq/docs/lda-vs-pca.html)\n",
        "\n",
        "### LDA\n",
        "[![](https://sebastianraschka.com/images/faq/lda-vs-pca/lda.png)](http://sebastianraschka.com/faq/docs/lda-vs-pca.html)\n",
        "\n",
        "Independent components analysis (ICA) is like PCA in that it can be run without labels (unsupervised). Instead of the directions of maximum variance, it finds the directions in which the data varies most independently.\n",
        "\n",
        "[![](http://phdthesis-bioinformatics-maxplanckinstitute-molecularplantphys.matthias-scholz.de/fig_pca_ica_independent_component_analysis.gif)](http://phdthesis-bioinformatics-maxplanckinstitute-molecularplantphys.matthias-scholz.de/#pca_ica_independent_component_analysis)\n",
        "[![](http://gael-varoquaux.info/science/attachments/ica_pca/ica_on_non_gaussian_data.png)](http://gael-varoquaux.info/science/ica_vs_pca.html)\n",
        "\n",
        "Here's what ICA looks like on the digits dataset:"
      ]
    },
    {
      "metadata": {
        "id": "xM3O-egdEdEH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_ica = decomposition.FastICA(n_components=2).fit_transform(X)\n",
        "plot_embedding(X_ica, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qs4u2sxqEdEL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The separation of the classes is closer to LDA than to PCA, but without any supervision. This isn't an inherent property of ICA or PCA, but it has more to do with the kind of data we're looking at.\n",
        "\n",
        "Besides these linear techniques, there are others called \"nonlinear dimensionality reduction algorithms\". Instead of imagining the projection like a shadow, imagine each point in high dimensional space like a particle get pushed and pulled independently. The goal is usually to end up with an embedding that keeps similar points close by, distant points far apart, and maintains some locally continuous behavior of variation.\n",
        "\n",
        "One of the most useful techniques for visualization purposes is called t-SNE (\"tee snee\"). A great paper showing comparisons is [here](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf). For a visualization of t-SNE see this [great article by Chris Olah](http://colah.github.io/posts/2014-10-Visualizing-MNIST/)."
      ]
    },
    {
      "metadata": {
        "id": "Bl0F4SUVEdEN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
        "%time X_tsne = tsne.fit_transform(X)\n",
        "plot_embedding(X_tsne, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H_R0HXpbEdET",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "But there are many others with their own benefits and disavantages. For example, Isomap has a very impressive demo of being able to \"unroll\" an extruded spiral (called a \"swiss roll\"), and to project images of hands in two dimensions that capture the two ways the hand gestures vary.\n",
        "\n",
        "![](http://web.mit.edu/cocosci/isomap/web1.jpg)\n",
        "\n",
        "![](http://web.mit.edu/cocosci/isomap/web2.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "3Q-6SMtaEdEU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%time X_iso = manifold.Isomap(n_neighbors, n_components=2).fit_transform(X)\n",
        "plot_embedding(X_iso, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cp5dGuWFEdEY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Some other algorithms are listed below."
      ]
    },
    {
      "metadata": {
        "id": "xat9NJ6pEdEZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2, method='standard')\n",
        "%time X_lle = clf.fit_transform(X)\n",
        "plot_embedding(X_lle, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a_5hFm26EdEc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2, method='modified')\n",
        "%time X_mlle = clf.fit_transform(X)\n",
        "plot_embedding(X_mlle, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pKo2kAo1EdEh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2, method='hessian')\n",
        "%time X_hlle = clf.fit_transform(X)\n",
        "plot_embedding(X_hlle, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i9GLggKOEdEl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2, method='ltsa')\n",
        "%time X_ltsa = clf.fit_transform(X)\n",
        "plot_embedding(X_ltsa, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8t9RqzGmEdEo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clf = manifold.MDS(n_components=2, n_init=1, max_iter=100)\n",
        "%time X_mds = clf.fit_transform(X)\n",
        "plot_embedding(X_mds, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9WGx7IeBEdEu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hasher = ensemble.RandomTreesEmbedding(n_estimators=200, random_state=0, max_depth=5)\n",
        "%time X_transformed = hasher.fit_transform(X)\n",
        "pca = decomposition.TruncatedSVD(n_components=2)\n",
        "%time X_reduced = pca.fit_transform(X_transformed)\n",
        "plot_embedding(X_reduced, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lR5l7bVwEdEx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedder = manifold.SpectralEmbedding(n_components=2, random_state=0, eigen_solver=\"arpack\")\n",
        "%time X_se = embedder.fit_transform(X)\n",
        "plot_embedding(X_se, y)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}