{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS01 word_embeddings.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnpharmd/DS-Sprint-01-Dealing-With-Data/blob/master/module1-afirstlookatdata/DS01_word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "9OLBNwhVDKL5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Using Pretrained Word Embeddings\n",
        "\n",
        "It turns out that are a ton of pretrained neural networks for accomplishing specific tasks. In this notebook, we'll explore what embeddings are and some of the cool things we can do with them once we have them.\n",
        "\n",
        "---\n",
        "\n",
        "## Representing Inputs\n",
        "### One-Hot Representation\n",
        "Traditionally, inputs to a neural network can be represented using a **One-Hot Representation**, where we assemble a vector of N values where each value represents a given class.\n",
        "\n",
        "For example, if we're classifying animals with our neural network, our one-hot representation may look like: < Bird, Cat, Dog, Fish > and <1,0,0,0> would represent a Bird. There are some problems with this. As the number of classes grows large, we waste a lot of space storing zeros that only tell us what we *don't* care about. Additionally, this gives us no way to represent the fact that Cats are more similar to Dogs than to Fish. \n",
        "\n",
        "In the Char-RNN notebook, we build a vocabulary of characters appearing in the training data and assign an ID to each one, letting Tensorflow handle the internal representation of the inputs. Because of this, our network learns only about what character should appear next based on the previous characters.\n",
        "\n",
        "### Embedding Representation\n",
        "An **embedding** is a representation of a value in a complex dataset in relation to the entire range of possible inputs based on some combination of features learned by the network. These can be a bit more abstract, which allows us to represent inputs as the neural network would rather than using firm classifications. Rather than a vector of on/off flags, we'll use a vector of floating point values, where each element represents the strength of a feature present in the input. One way we can generate an embedding vector is to select a layer in our neural network before the output layer and look at the values produced by it.\n",
        "\n",
        "---\n",
        "## Further Reading\n",
        "If you're interested in learning more about how word embeddings can be trained, there's a great tutorial about [Word2Vec](https://www.tensorflow.org/tutorials/word2vec) on tensorflow.org or [this series of blog posts by Memo Akten](https://medium.com/artists-and-machine-intelligence/ami-residency-part-1-exploring-word-space-andprojecting-meaning-onto-noise-98af7252f749)\n",
        "\n",
        "Embedding vectors have all sorts of applications across various types of data. \n",
        "* [Here's](https://artsexperiments.withgoogle.com/tsnemap/) one example where works of art have been mapped by their similarity, which lets us [morph from one work to another](https://artsexperiments.withgoogle.com/xdegrees)."
      ]
    },
    {
      "metadata": {
        "id": "-w0nSgcgWqOd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using pip to install gensim\n",
        "[Gensim](https://radimrehurek.com/gensim/) is a Python library that makes it very easy to work with word embeddings. This cell may take a few moments to run as it is installed."
      ]
    },
    {
      "metadata": {
        "id": "gZRWEtDzC_zy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q gensim==3.2.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fRk43NZ2XXgG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Downloading our pre-trained word embeddings\n",
        "We're going to download a set of three million pretrained English word embeddings from a Word2Vec model that was trained on Google News. Some information from the project can be found on [this page](https://code.google.com/archive/p/word2vec/) along with the link to the Google Drive link. The next few cells will take care of this process for you."
      ]
    },
    {
      "metadata": {
        "id": "WWEwhZhEZS2L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "file_id = '0B7XkCwpI5KDYNlNUTTlSS21pQmM'\n",
        "downloaded = drive.CreateFile({'id':file_id})\n",
        "downloaded.FetchMetadata(fetch_all=True)\n",
        "downloaded.GetContentFile(downloaded.metadata['title'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ITYFBY10ZX2q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!gunzip GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kjh8HsQQZb-p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading the embeddings into Gensim\n",
        "The next cell will load word embeddings for the two hundred thousand most common words in English. \n",
        "\n",
        "The dataset will not contain excessively common \"stop words\" such as 'and', and others. These sorts of words don't really add to the meaning of the other words in a text and so are often omitted while training word embeddings.\n",
        "\n",
        "While the dataset contains nearly three million words, we're going to cut it down to size so you don't have to wait as long."
      ]
    },
    {
      "metadata": {
        "id": "vF2yHZGzZnNL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "gensim_model = KeyedVectors.load_word2vec_format(\n",
        "    'GoogleNews-vectors-negative300.bin', binary=True, limit=300000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lz1fVGJw2Ha-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here's an example of how a word is represented in our embedding space:"
      ]
    },
    {
      "metadata": {
        "id": "HQTa3sXE2G3v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('hello =', gensim_model['hello'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZP-JcBsyg85c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Whoops, that's really dense. Visualizing our embeddings can help us draw conclusions about our dataset and gain some insight into what our neural network is learning.\n",
        "\n",
        "T-SNE is an algorithm that reduces the dimensionality of our embedding vectors. The Google News Word2Vec embeddings are originally 300-dimensional, but T-Sne will let us view them collapsed into a 2D space based on their similarities."
      ]
    },
    {
      "metadata": {
        "id": "j29BEwyHg7Z0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from matplotlib import pylab\n",
        "\n",
        "words = [word for word in gensim_model.index2word[200:600]]\n",
        "embeddings = [gensim_model[word] for word in words]\n",
        "words_embedded = TSNE(n_components=2).fit_transform(embeddings)\n",
        "\n",
        "pylab.figure(figsize=(20, 20))\n",
        "for i, label in enumerate(words):\n",
        "  x, y = words_embedded[i, :]\n",
        "  pylab.scatter(x, y)\n",
        "  pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',\n",
        "                 ha='right', va='bottom')\n",
        "pylab.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XxOI7mLd0A_5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that from looking at the graph above we can see that words with similar usages appear near each other. For example,\n",
        "* Words dealing with time are next to each other (minutes, hours, months, weeks, days)\n",
        "* Countries appear near each other (India, China, United_States)\n",
        "* US Presidents (Bush, Obama)\n",
        "* Words associated with games or sports (playing, played, player, coach, gamers, players, teams)\n",
        "* Numbers (2, 3, 4, 5, 6, 7, 8, 9)\n",
        "* Directions (North, South)\n",
        "* Words related to probability (whether, might, expected, never, sure, possible)\n",
        "\n",
        "Can you find any other interesting ones? Try changing the [200:600] in the cell above to try another range of values.\n"
      ]
    },
    {
      "metadata": {
        "id": "J4pVb7fB37U5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Interesting applications of word embeddings\n",
        "\n",
        "Now that we have some idea of what knowledge our neural network has, let's explore some of the more interesting implications."
      ]
    },
    {
      "metadata": {
        "id": "53bSTEQ052_R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Finding similar words:"
      ]
    },
    {
      "metadata": {
        "id": "D8E1w0Dv4TPq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gensim_model.most_similar(positive=['January'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uc53XRDpao5W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Combining Words:\n",
        "Since our words are represented as an array of floats, we can add some together and lookup what their combination is.\n",
        "\n",
        "```\n",
        "nature + science = biology, ecology\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "fXwivSYUasaJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gensim_model.most_similar(positive=['nature', 'science'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WXhtNUE86GUA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Finding analogous words:\n",
        "\n",
        "We can do math with the embedding vectors for words to find analogies between words.\n",
        "\n",
        "king - man + woman = queen\n",
        "\n",
        "Paris - France + Germany = Berlin\n",
        "\n",
        "Tea - England + United_States = Coffee\n",
        "\n",
        "North - South + West = East"
      ]
    },
    {
      "metadata": {
        "id": "B76r13IN7Yhn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gensim_model.most_similar(positive=['king', 'woman'], negative=['man'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iwv2bXg1BCWj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gensim_model.most_similar(\n",
        "    positive=['Paris', 'Spain'], \n",
        "    negative=['France'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iNfeZ_Uc9zkm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gensim_model.most_similar(positive=['Tea', 'United_States'], negative=['England'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vE_xoDVPBTdP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gensim_model.most_similar(positive=['North', 'West'], negative=['South'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WsbpjwR0n5cP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can put these together to programmatically modify sentences and phrases word by word, with varying results:"
      ]
    },
    {
      "metadata": {
        "id": "WnIZs1oioIWV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def shift_context(sentence, from_context, to_context):\n",
        "  new_sentence = []\n",
        "  for word in sentence.split():\n",
        "    if word in gensim_model:\n",
        "      word = gensim_model.most_similar(\n",
        "          positive=[word, to_context], negative=[from_context])[0][0]\n",
        "    new_sentence.append(word)\n",
        "\n",
        "  return ' '.join(new_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t5-5lLJIvsL1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentence = 'restaurant serving coffee with cream and bread'\n",
        "print(shift_context(sentence, 'regular', 'fancy'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WP2e_sOWzH7-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Unfortunately, the results are not always perfect with this approach, for example:\n",
        "\n",
        "excellent - positive + negative = \n",
        "\n",
        "```\n",
        "[(u'terrific', 0.5454081296920776),\n",
        " (u'superb', 0.5449916124343872),\n",
        " (u'exceptional', 0.5175209641456604),\n",
        " (u'Excellent', 0.4948967695236206),\n",
        " (u'impeccable', 0.49398699402809143),\n",
        " (u'superlative', 0.4694099426269531),\n",
        " (u'ideal', 0.4649601876735687),\n",
        " (u'fantastic', 0.46219557523727417),\n",
        " (u'abysmal', 0.4582980275154114),\n",
        " (u'atrocious', 0.45645347237586975)]\n",
        "```\n",
        " The first eight results have roughly the same meaning as excellent. An unfortunate fact is that words can have many meanings across different contexts. Using these word embeddings, \n",
        " \n",
        " ```plus - positive + negative = minus```. So the words positive and negative have become associated with mathematical sign rather than good and bad.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "pKex2of82mjL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Other Problems and Strategies"
      ]
    },
    {
      "metadata": {
        "id": "8XLYNLirEYx0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We run into an issue when attempting the following:\n",
        "\n",
        "\n",
        "```\n",
        "Austin - Texas + Oregon\n",
        "```\n",
        "Which yields \"Corvallis\" when we should expect \"Salem\". One way around this is to determine what relationship we want to target, in this case the full list of US States and their Capitals, and compute the average vector between our embeddings for this relationship. Gensim will let us do this by adding more parameters to the positive and negative lists.\n"
      ]
    },
    {
      "metadata": {
        "id": "WSLBC6jxRxYU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gensim_model.most_similar(\n",
        "    positive=['Austin', 'Oregon'],\n",
        "    negative=['Texas']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m6hBiVuXGIey",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gensim_model.most_similar(\n",
        "    positive=[\n",
        "        'Austin', 'Atlanta', 'Nashville', 'Sacramento', 'Boston', 'Oregon'\n",
        "    ],\n",
        "    negative=[\n",
        "        'Texas', 'Georgia', 'Tennessee', 'California', 'Massachusetts',\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1B3LhjppVlpZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "While still not the answer we were looking for, we can now see that our model has learned about the relationship between major cities and their US states in general rather than state capitals. One explanation for this could be that more populous cities are more likely to be featured in news articles when a state is mentioned."
      ]
    }
  ]
}