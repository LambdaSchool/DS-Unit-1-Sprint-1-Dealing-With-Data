{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_111_A_First_Look_at_Data.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChanceDurr/DS-Unit-1-Sprint-1-Dealing-With-Data/blob/master/module1-afirstlookatdata/LS_DS_111_A_First_Look_at_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Okfr_uhwhS1X",
        "colab_type": "text"
      },
      "source": [
        "# Lambda School Data Science - A First Look at Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dtJETFRhnOG",
        "colab_type": "text"
      },
      "source": [
        "## Lecture - let's explore Python DS libraries and examples!\n",
        "\n",
        "The Python Data Science ecosystem is huge. You've seen some of the big pieces - pandas, scikit-learn, matplotlib. What parts do you want to see more of?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiBkgmPJhmhE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ef7ea3e-e5f6-4db0-8e17-1b7fd1126794"
      },
      "source": [
        "# TODO - we'll be doing this live, taking requests\n",
        "# and reproducing what it is to look up and learn things\n",
        "\n",
        "#This is a change to the notebook\n",
        "2 + 2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOqaPds9huME",
        "colab_type": "text"
      },
      "source": [
        "## Assignment - now it's your turn\n",
        "\n",
        "Pick at least one Python DS library, and using documentation/examples reproduce in this notebook something cool. It's OK if you don't fully understand it or get it 100% working, but do put in effort and look things up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGUS79cOhPWj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c1a7a197-1a22-4f54-9c67-edea32426403"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "\n",
        "def wordOnPage(url, word):\n",
        "  page = requests.get(url).content #Use requests to get the content of the page\n",
        "  soup = BeautifulSoup(page, 'html.parser') #input the content of the request into beautiful soup using python's html parser\n",
        "\n",
        "  words = list((soup.get_text().split(\" \"))) #take all the text on the page and make each word a value in a list\n",
        "\n",
        "  count = 0 #start the count var at 0\n",
        "\n",
        "  for i in words: # for each time that word appears in the list, add 1 to the count\n",
        "    if i.upper() == word.upper(): #using the .upper() function to get rid of capitalization discrepencies\n",
        "      count += 1\n",
        "  \n",
        "  return f\"The word \\'{word}\\' is on the page {count} times\" # Return a string that tells you how many time a given word appears on a given page\n",
        "\n",
        "website = 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'\n",
        "\n",
        "wordOnPage(website, 'Beautiful')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The word 'Beautiful' is on the page 108 times\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT9gdS7viJZa",
        "colab_type": "text"
      },
      "source": [
        "### Assignment questions\n",
        "\n",
        "After you've worked on some code, answer the following questions in this text block:\n",
        "\n",
        "1.  Describe in a paragraph of text what you did and why, as if you were writing an email to somebody interested but nontechnical.\n",
        "\n",
        "2.  What was the most challenging part of what you did?\n",
        "\n",
        "3.  What was the most interesting thing you learned?\n",
        "\n",
        "4.  What area would you like to explore with more time?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk91Rcjo8TFD",
        "colab_type": "text"
      },
      "source": [
        "    In the code above, what I did was create a '*Program*' that take a website page and a word that you give it and it tells you how many times that word appears on that page. I chose to use Beautiful Soup because I kept hearing about it and decided to look into what it was. From what I found, I also needed to use requests to get the HTML of the page to use in beautiful soup.\n",
        "    \n",
        "    The most challenging part of what I did was getting through the documentation to figure out what I needed to do to get the content of the page to appear in a str so that I could make a list of all the words on the page.\n",
        "    \n",
        "    The most interesting thing I learned probably was the requests part. It looks like you can do a lot with the GET and POST functions. I'm not exactly sure how they work, but I have a slight idea. \n",
        "    \n",
        "    Based off of what I just worked on, I would like to explore more on website scraping to get data to create insights. \n",
        "\n",
        "  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XXg2crAipwP",
        "colab_type": "text"
      },
      "source": [
        "## Stretch goals and resources\n",
        "\n",
        "Following are *optional* things for you to take a look at. Focus on the above assignment first, and make sure to commit and push your changes to GitHub (and since this is the first assignment of the sprint, open a PR as well).\n",
        "\n",
        "- [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/)\n",
        "- [scikit-learn documentation](http://scikit-learn.org/stable/documentation.html)\n",
        "- [matplotlib documentation](https://matplotlib.org/contents.html)\n",
        "- [Awesome Data Science](https://github.com/bulutyazilim/awesome-datascience) - a list of many types of DS resources\n",
        "\n",
        "Stretch goals:\n",
        "\n",
        "- Find and read blogs, walkthroughs, and other examples of people working through cool things with data science - and share with your classmates!\n",
        "- Write a blog post (Medium is a popular place to publish) introducing yourself as somebody learning data science, and talking about what you've learned already and what you're excited to learn more about."
      ]
    }
  ]
}